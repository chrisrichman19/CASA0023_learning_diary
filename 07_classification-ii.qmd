# Classification II

## Summary

This week, we explored accuracy measures for classification and will use practical applications of SVM and random forest models to explain different accuracy metrics.

In last week's practical, I focused on Bristol, my undergraduate city. I built a simple random forest model using a limited training dataset (3-5 polygons per land type), which included categories such as forest, low urban, high urban, water, and bare earth. For this week, I found SVM code on Google Earth Engine and adapted it for my analysis to compare the accuracy measures across each method.

Random Forest Classifier:

![](images/clipboard-4120096112.png){width="481"}

Support Vector Machine (SVM):

![](images/clipboard-4234707019.png){width="475"}

Light pink: Low urban\
Blue: Water\
Dark pink: High urban\
Beige: Grass\
Grey: Bare earth\
Green: Forest\
\

Visually, SVM predicts that there is a lot more forest in what the RF predicts as low urban. But how can we interpret these results? There are various measures each with their own advantages and disadvantages.

**Accuracy Assessment**

Each predicted value is placed into this Confusion Matrix, which acts as the foundation for the upcoming measures.

![](images/clipboard-3707637320.png)

**Overall accuracy** measures the total correct prediction in proportion to all prediction

$$
  \frac{\text{Number of Correct Predictions}}{\text{Total Predictions}} = \frac{TP + TN}{TP + FP + FN + TN}
  $$

-   **RF**: 0.981 \| **SVM**: 0.879

RF’s high overall accuracy suggests strong general performance across all classes. SVM’s lower score may stem from misclassifications, especially in overlapping classes

#### **Class-Specific Metrics (Low Urban Class)**

**Producer accuracy** measures the true positive rate in a class or the proportion of correctly identified positives relative to actual positives. I will just look at low urban class, so the number of pixels correctly identified pixels as low urban divided by the total pixels in the low urban class.

$$
  \text{Producer Accuracy} = \frac{TP}{TP + FN}
  $$ **RF**: 0.943 \| **SVM**: 0.478

SVM misses \~52% of true low urban pixels (e.g., misclassifying them as forest). RF captures \~94% of actual low urban areas.

**User accuracy** measures the proportion of correctly predicted positives relative to all predicted positives.

$$
  \text{User Accuracy} = \frac{TP}{TP + FP}
  $$

Again, using the low urban class - the number of pixels correctly identified as low urban divided by the total number of pixels that are claimed to be low urban.

**RF**: 0.955 \| **SVM**: 0.962

SVM’s and RF's high User Accuracy means its low urban predictions are reliable here (few false positives).

There is also **F1 score** which is the harmonic mean of precision (UA) and recall (PA). Useful for balancing UA and PA, especially in imbalanced datasets like our own.

**RF**: 0.949 \| **SVM**: 0.639

#### **Additional Metrics**

**Kappa** **coefficient** is designed to express the accuracy of an image compared to the results done by chance. Ranges from 0 to 1. calls to abandon it as it may not be that useful

$$
  \kappa = \frac{\text{Overall Accuracy} - \text{Expected Accuracy}}{1 - \text{Expected Accuracy}}
  $$

Where expected accuracy =

$$ 
(\frac{\sum (\text{Row Total} \times \text{Column Total})}{N^2})
$$

**RF**: 0.977 \| **SVM**: 0.855

Both models perform better than random chance, but Kappa is debated due to sensitivity to class distribution.

## Application

Whilst traditional machine learning techniques like Support Vector Machines and Random Forests are powerful, there are some more complicated classification use-case in remote sensing where more advanced tools are required [@song2019].

One example is object detection which involves identifying the locations and types of specific targets within an image and labeling them with bounding boxes. The use of Convolutional Neural Networks has revolutionised how objects are detected and classified in satellite imagery [@yang].

Early remote sensing workflows relied heavily on manual feature engineering, where texture and spectral signatures were used to distinguish targets like ships or landslides from their surroundings. However, as noted by [@yang], these methods struggled with the "rich, diverse, and detailed" nature of remote sensing imagery, particularly for small or irregularly shaped objects. For instance, in ship detection, spectral thresholds often failed to differentiate ships from wave patterns or coastal clutter, leading to high false-positive rates [@wu2018].

Wu et al. (2018) demonstrate how CNNs overcome these shortcomings in their study [@wu2018].

![](images/clipboard-3768825483.png)

A CNN indentifies ship heads and estimates their orientation, bypassing background noise, leveraging CNNs’ ability to learn structural patterns automatically.

A second CNN then uses the ship head’s orientation to propose regions for the full ship. It then distinguishes true ships from false positives (e.g., buildings, wave crests) by analysing relationships between the ship head and surrounding pixels.

Unlike SVM or Random forest models, Wu et al.'s CNNs autonomously learns features, eliminating the need for manual feature engineering.

## Reflection

In land classification, traditional cross-validation (e.g., k-fold) risks overestimating model accuracy in spatially autocorrelated data, as nearby training and test samples may share patterns (e.g., within the same land parcel), inflating performance. Spatial cross-validation (CV) addresses this by partitioning data into geographically disjoint folds, ensuring that training and test sets are spatially independent.

**Reflection on Model Selection:**\
Convolutional Neural Networks (CNNs) are generally more accurate for complex tasks like object detection but they demand substantial labeled data and computational resources. Random Forests (RF) and Support Vector Machines (SVM) are more efficient and interpretable, but struggle with complex spatial patterns.
